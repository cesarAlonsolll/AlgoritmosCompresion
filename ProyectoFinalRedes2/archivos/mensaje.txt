ades. Ya
Los dos primeros por ser los más sencillos, pero aún así siendo Huffman uno de los más utilizados en los estándares actuales, y el tercero por ser en concepto diferente a los 2 primeros y también ser uno de los más importantes en la industria de la compresión de archivos. Todos son métodos de compresión sin pérdida, por lo que la información se mantendrá intacta después de haber sido comprimida y podrá ser recuperada íntegramente en el proceso de descompresión.

Shannon Fano:

La codificación en este método se hace mediante la reducción de bits que representan a los símbolos  naturales, tales como las letras del alfabetos, los caracteres chinos, etc. Normalmente cada uno de estos símbolos está codificado en una palabra de 8 bits de longitud, por lo que si cada palabra tiene la misma longitud, pero unas se usan más frecuentemente que otras entonces se estará desperdiciando espacio, ya que si se disminuyera el tamaño de las más usadas y en compensación se aumentará el de las menos usadas se ahorraría una cantidad significativa de memoria y almacenamiento.

Primero se debe generar una lista con las probabilidades de uso de cada uno de los símbolos y ordenarla de el más probable a menos probable. Se puede tener una lista preestablecida ya fijada que se usara con todos los códigos o generar una adaptada a cada uno de los códigos que se vayan a comprimir. Al tener la lista hecha se procederá a dividirla en 2 partes, de tal manera que en ambas la suma de las probabilidades de todos los caracteres en cada una sean lo más similares posibles. Después a cada una de las lista se le procederá a asignar un 1 o un 0, 1 para la que tenga la mayor acumulación de probabilidad y 0 para la que tenga menos, y luego a todos sus componentes se le asignará ese valor como el primer símbolo binario de la palabra que los representará. Se realiza el mismo procedimiento a cada una de las sublistas creadas hasta que solo quede 1 elemento en todas las listas, lo que dará fin al algoritmo.

La decodificación de un código generado por este algoritmo se hace a través de la tabla que se halla producido, buscando de izquierda a derecha coincidencias entre las palabras del documento y las almacenadas en la tabla.

Huffman:

Muy similar a Shannon Fano, ya que posee el mismo principio detrás de su algoritmo de reducir las palabras de los símbolos más utilizados, y por lo tanto también hace uso de listas con probabilidades, pero el desarrollo del algoritmo se diferencia desde aquí. Al iniciar el algoritmo se escogerán los 2 caracteres con la menor probabilidad y se acomodarán en una estructura de árbol binaria quedando como las hojas y el nodo padre tendría el acumulado de sus probabilidades. Ya que se está trabajando en un sistema en base 2 y el máximo de símbolos disponibles para formar una palabra son 2 el árbol debe ser uno binario, si la base fuera mayor el árbol generado tendría ramificaciones en una cantidad acorde a esa base, si es 3 tendría 3. Ahora el nodo será tomado como un carácter individual, pero tendrá acumulada la probabilidad de sus 2 hojas, que bien estas podrían ser caracteres únicos u otros nodos. Desde aquí se retomará el algoritmo desde el inicio hasta que todos los caracteres sean acomodados en la estructura.